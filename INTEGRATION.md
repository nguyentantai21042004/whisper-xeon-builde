# Whisper Xeon Artifacts - Integration Guide

H∆∞·ªõng d·∫´n t√≠ch h·ª£p Whisper artifacts t·ª´ MinIO v√†o c√°c d·ª± √°n kh√°c.

## üì¶ Artifacts C√≥ G√¨?

Sau khi build, artifacts ƒë∆∞·ª£c l∆∞u tr√™n MinIO v·ªõi c·∫•u tr√∫c:

```
whisper-artifacts/
‚îú‚îÄ‚îÄ whisper_small_xeon/
‚îÇ   ‚îú‚îÄ‚îÄ libwhisper.so          # Th∆∞ vi·ªán Whisper C++ (540 KB)
‚îÇ   ‚îú‚îÄ‚îÄ libggml.so.0           # GGML core (47 KB)
‚îÇ   ‚îú‚îÄ‚îÄ libggml-base.so.0      # GGML base (625 KB)
‚îÇ   ‚îú‚îÄ‚îÄ libggml-cpu.so.0       # GGML CPU backend (649 KB)
‚îÇ   ‚îú‚îÄ‚îÄ ggml-small-q5_1.bin    # Model Small quantized (181 MB)
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ
‚îî‚îÄ‚îÄ whisper_medium_xeon/
    ‚îú‚îÄ‚îÄ libwhisper.so          # Th∆∞ vi·ªán Whisper C++ (540 KB)
    ‚îú‚îÄ‚îÄ libggml.so.0           # GGML core (47 KB)
    ‚îú‚îÄ‚îÄ libggml-base.so.0      # GGML base (625 KB)
    ‚îú‚îÄ‚îÄ libggml-cpu.so.0       # GGML CPU backend (649 KB)
    ‚îú‚îÄ‚îÄ ggml-medium-q5_1.bin   # Model Medium quantized (1.5 GB)
    ‚îî‚îÄ‚îÄ README.md
```

### Model Specifications

| Model | Parameters | Quantized Size | Speed | Accuracy | Use Case |
|-------|-----------|----------------|-------|----------|----------|
| **Small** | ~244M | 181 MB | Fast | Good | Real-time transcription, quick processing |
| **Medium** | ~769M | 1.5 GB | Moderate | Better | Higher accuracy requirements |

### Library Dependencies

C√°c file `.so` c√≥ dependencies:
- `libwhisper.so` ‚Üí c·∫ßn `libggml.so.0`
- `libggml.so.0` ‚Üí c·∫ßn `libggml-base.so.0` v√† `libggml-cpu.so.0`

## üîß System Requirements

**Runtime Dependencies:**
- Linux x86_64 (Ubuntu 20.04+, CentOS 8+, Debian 11+)
- CPU v·ªõi AVX2 v√† FMA support (Intel Xeon, Core i5/i7/i9 4th gen+)
- `libgomp1` (OpenMP runtime)
- Python 3.8+ (cho Python integration)

**Ki·ªÉm tra CPU support:**
```bash
grep -E "avx2|fma" /proc/cpuinfo
```

**C√†i ƒë·∫∑t dependencies:**
```bash
# Ubuntu/Debian
sudo apt-get update
sudo apt-get install -y libgomp1

# CentOS/RHEL
sudo yum install -y libgomp
```

## üì• Download Artifacts t·ª´ MinIO

### Option 1: S·ª≠ d·ª•ng Python Script (Recommended)

T·∫°o file `download_whisper_artifacts.py`:

```python
#!/usr/bin/env python3
"""
Download Whisper artifacts from MinIO
"""
import os
import sys
from pathlib import Path

try:
    import boto3
    from botocore.exceptions import ClientError
    from botocore.client import Config
except ImportError:
    print("Error: boto3 not installed. Install with: pip install boto3")
    sys.exit(1)

# MinIO Configuration
MINIO_ENDPOINT = "http://172.16.19.115:9000"  # Thay ƒë·ªïi cho ph√π h·ª£p
MINIO_ACCESS_KEY = "smap"
MINIO_SECRET_KEY = "hcmut2025"
BUCKET_NAME = "whisper-artifacts"

# Ch·ªçn model (small ho·∫∑c medium)
MODEL_SIZE = "small"  # ho·∫∑c "medium"


def download_artifacts(model_size="small"):
    """Download Whisper artifacts cho m·ªôt model size"""
    
    # Create output directory
    output_dir = Path(f"whisper_{model_size}_xeon")
    output_dir.mkdir(exist_ok=True)
    
    print(f"üì¶ Downloading Whisper {model_size.upper()} artifacts...")
    print(f"   Target: {output_dir}/")
    print()
    
    # Create S3 client
    s3_client = boto3.client(
        's3',
        endpoint_url=MINIO_ENDPOINT,
        aws_access_key_id=MINIO_ACCESS_KEY,
        aws_secret_access_key=MINIO_SECRET_KEY,
        config=Config(signature_version='s3v4'),
        region_name='us-east-1'
    )
    
    # List of files to download
    prefix = f"whisper_{model_size}_xeon/"
    
    try:
        # List objects in bucket
        response = s3_client.list_objects_v2(Bucket=BUCKET_NAME, Prefix=prefix)
        
        if 'Contents' not in response:
            print(f"‚ùå No artifacts found for {model_size} model")
            return False
        
        # Download each file
        for obj in response['Contents']:
            key = obj['Key']
            filename = key.split('/')[-1]
            
            if not filename:  # Skip directory entries
                continue
            
            local_path = output_dir / filename
            file_size_mb = obj['Size'] / (1024 * 1024)
            
            print(f"‚¨áÔ∏è  {filename} ({file_size_mb:.1f} MB)...", end=" ", flush=True)
            
            try:
                s3_client.download_file(BUCKET_NAME, key, str(local_path))
                print("‚úì")
            except ClientError as e:
                print(f"‚úó Error: {e}")
                return False
        
        print()
        print(f"‚úÖ Downloaded to: {output_dir}/")
        return True
        
    except ClientError as e:
        print(f"‚ùå Error accessing MinIO: {e}")
        return False


if __name__ == "__main__":
    if len(sys.argv) > 1:
        MODEL_SIZE = sys.argv[1].lower()
    
    if MODEL_SIZE not in ["small", "medium"]:
        print("Usage: python download_whisper_artifacts.py [small|medium]")
        sys.exit(1)
    
    success = download_artifacts(MODEL_SIZE)
    sys.exit(0 if success else 1)
```

**S·ª≠ d·ª•ng:**

```bash
# C√†i boto3
pip install boto3

# Download Small model
python download_whisper_artifacts.py small

# Download Medium model
python download_whisper_artifacts.py medium
```

### Option 2: S·ª≠ d·ª•ng mc (MinIO Client)

```bash
# C√†i ƒë·∫∑t mc
wget https://dl.min.io/client/mc/release/linux-amd64/mc
chmod +x mc
sudo mv mc /usr/local/bin/

# Configure MinIO
mc alias set myminio http://172.16.19.115:9000 smap hcmut2025

# Download Small model
mc cp --recursive myminio/whisper-artifacts/whisper_small_xeon/ ./whisper_small_xeon/

# Download Medium model
mc cp --recursive myminio/whisper-artifacts/whisper_medium_xeon/ ./whisper_medium_xeon/
```

### Option 3: S·ª≠ d·ª•ng curl (Manual)

```bash
# Download t·ª´ng file b·∫±ng presigned URL
# (C·∫ßn generate presigned URL t·ª´ MinIO Console ho·∫∑c API)
```

## üêç Python Integration

### Setup

```bash
# T·∫°o virtual environment
python3 -m venv venv
source venv/bin/activate

# C√†i c√°c package c·∫ßn thi·∫øt
pip install numpy scipy
```

### Basic Usage v·ªõi ctypes

```python
import ctypes
import os
from pathlib import Path

# Load libraries v·ªõi ƒë√∫ng th·ª© t·ª± dependencies
lib_dir = Path("whisper_small_xeon")

# Set LD_LIBRARY_PATH ƒë·ªÉ t√¨m dependencies
os.environ['LD_LIBRARY_PATH'] = str(lib_dir) + ':' + os.environ.get('LD_LIBRARY_PATH', '')

# Load dependencies tr∆∞·ªõc
libggml_base = ctypes.CDLL(str(lib_dir / "libggml-base.so.0"), mode=ctypes.RTLD_GLOBAL)
libggml_cpu = ctypes.CDLL(str(lib_dir / "libggml-cpu.so.0"), mode=ctypes.RTLD_GLOBAL)
libggml = ctypes.CDLL(str(lib_dir / "libggml.so.0"), mode=ctypes.RTLD_GLOBAL)

# Load Whisper
libwhisper = ctypes.CDLL(str(lib_dir / "libwhisper.so"))

# Initialize context
model_path = str(lib_dir / "ggml-small-q5_1.bin")
ctx = libwhisper.whisper_init_from_file(model_path.encode('utf-8'))

if ctx:
    print("‚úì Whisper initialized successfully!")
    
    # S·ª≠ d·ª•ng Whisper API
    # (Xem include/whisper.h ƒë·ªÉ bi·∫øt c√°c functions c√≥ s·∫µn)
    
    # Free context khi xong
    libwhisper.whisper_free(ctx)
else:
    print("‚úó Failed to initialize Whisper")
```

### Advanced: Wrapper Class

```python
import ctypes
import numpy as np
from pathlib import Path

class WhisperTranscriber:
    """Python wrapper cho Whisper C++ library"""
    
    def __init__(self, model_dir="whisper_small_xeon"):
        self.lib_dir = Path(model_dir)
        self._load_libraries()
        self._init_context()
    
    def _load_libraries(self):
        """Load all required libraries"""
        # Pre-load dependencies
        ctypes.CDLL(str(self.lib_dir / "libggml-base.so.0"), mode=ctypes.RTLD_GLOBAL)
        ctypes.CDLL(str(self.lib_dir / "libggml-cpu.so.0"), mode=ctypes.RTLD_GLOBAL)
        ctypes.CDLL(str(self.lib_dir / "libggml.so.0"), mode=ctypes.RTLD_GLOBAL)
        
        # Load Whisper
        self.lib = ctypes.CDLL(str(self.lib_dir / "libwhisper.so"))
        
        # Define function signatures
        self.lib.whisper_init_from_file.argtypes = [ctypes.c_char_p]
        self.lib.whisper_init_from_file.restype = ctypes.c_void_p
        
        self.lib.whisper_free.argtypes = [ctypes.c_void_p]
        self.lib.whisper_free.restype = None
    
    def _init_context(self):
        """Initialize Whisper context"""
        model_path = str(self.lib_dir / "ggml-small-q5_1.bin")
        self.ctx = self.lib.whisper_init_from_file(model_path.encode('utf-8'))
        
        if not self.ctx:
            raise RuntimeError("Failed to initialize Whisper context")
    
    def transcribe(self, audio_file):
        """
        Transcribe audio file
        
        Args:
            audio_file: Path to WAV file (16kHz, mono)
        
        Returns:
            str: Transcribed text
        """
        # TODO: Implement transcription logic
        # See whisper.h for full API
        pass
    
    def __del__(self):
        """Cleanup"""
        if hasattr(self, 'ctx') and self.ctx:
            self.lib.whisper_free(self.ctx)

# Usage
transcriber = WhisperTranscriber("whisper_small_xeon")
# result = transcriber.transcribe("audio.wav")
```

## üîß C/C++ Integration

### CMakeLists.txt

```cmake
cmake_minimum_required(VERSION 3.10)
project(MyWhisperApp)

set(CMAKE_CXX_STANDARD 17)

# Whisper artifacts location
set(WHISPER_DIR "${CMAKE_SOURCE_DIR}/whisper_small_xeon")

# Include Whisper header (download t·ª´ repo)
include_directories(${CMAKE_SOURCE_DIR}/include)

# Link libraries
link_directories(${WHISPER_DIR})

add_executable(my_app main.cpp)

target_link_libraries(my_app
    ${WHISPER_DIR}/libwhisper.so
    ${WHISPER_DIR}/libggml.so.0
    ${WHISPER_DIR}/libggml-base.so.0
    ${WHISPER_DIR}/libggml-cpu.so.0
    pthread
    m
)

# Set RPATH ƒë·ªÉ t√¨m .so files
set_target_properties(my_app PROPERTIES
    BUILD_RPATH "${WHISPER_DIR}"
    INSTALL_RPATH "${WHISPER_DIR}"
)
```

### main.cpp

```cpp
#include <iostream>
#include "whisper.h"

int main() {
    // Initialize Whisper
    const char* model_path = "whisper_small_xeon/ggml-small-q5_1.bin";
    struct whisper_context* ctx = whisper_init_from_file(model_path);
    
    if (!ctx) {
        std::cerr << "Failed to initialize Whisper" << std::endl;
        return 1;
    }
    
    std::cout << "Whisper initialized successfully!" << std::endl;
    
    // Use Whisper API
    // ...
    
    // Cleanup
    whisper_free(ctx);
    return 0;
}
```

### Build & Run

```bash
# Download whisper.h header
wget https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/include/whisper.h -P include/

# Build
mkdir build && cd build
cmake ..
make

# Run
./my_app
```

## üê≥ Docker Integration

### Dockerfile

```dockerfile
FROM ubuntu:22.04

# Install runtime dependencies
RUN apt-get update && apt-get install -y \
    libgomp1 \
    python3 \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# Install boto3 for downloading
RUN pip3 install boto3

# Set working directory
WORKDIR /app

# Copy download script
COPY download_whisper_artifacts.py .

# Download artifacts at build time (ho·∫∑c runtime)
ARG MODEL_SIZE=small
RUN python3 download_whisper_artifacts.py ${MODEL_SIZE}

# Copy your application
COPY . .

# Set library path
ENV LD_LIBRARY_PATH=/app/whisper_${MODEL_SIZE}_xeon:$LD_LIBRARY_PATH

CMD ["python3", "your_app.py"]
```

### docker-compose.yml

```yaml
version: '3.8'

services:
  whisper-app:
    build:
      context: .
      args:
        MODEL_SIZE: small
    environment:
      - LD_LIBRARY_PATH=/app/whisper_small_xeon
    volumes:
      - ./data:/app/data
    ports:
      - "8000:8000"
```

## üìã Checklist T√≠ch H·ª£p

- [ ] Ki·ªÉm tra CPU support (AVX2, FMA)
- [ ] C√†i ƒë·∫∑t `libgomp1`
- [ ] Download artifacts t·ª´ MinIO (ch·ªçn Small ho·∫∑c Medium)
- [ ] Verify c√°c file `.so` v√† `.bin` ƒë√£ t·∫£i v·ªÅ
- [ ] Set `LD_LIBRARY_PATH` ho·∫∑c RPATH
- [ ] Test load libraries th√†nh c√¥ng
- [ ] Test transcribe v·ªõi sample audio
- [ ] Optimize cho production (caching, threading, etc.)

## ‚ö° Performance Tips

### 1. Model Selection
- **Small model**: D√πng cho real-time, latency-sensitive applications
- **Medium model**: D√πng khi c·∫ßn accuracy cao h∆°n, c√≥ th·ªÉ ch·∫•p nh·∫≠n latency

### 2. Thread Configuration
```python
# Set s·ªë threads cho Whisper
os.environ['OMP_NUM_THREADS'] = '4'  # Adjust based on CPU cores
```

### 3. Batch Processing
- Process nhi·ªÅu audio files trong m·ªôt batch
- Reuse Whisper context thay v√¨ init m·ªói l·∫ßn

### 4. Memory Management
- Small model: ~500 MB RAM
- Medium model: ~2 GB RAM
- C·∫ßn th√™m RAM cho audio buffer

## üêõ Troubleshooting

### Library Not Found Error

```
Error: libwhisper.so: cannot open shared object file
```

**Fix:**
```bash
export LD_LIBRARY_PATH=/path/to/whisper_small_xeon:$LD_LIBRARY_PATH
```

### CPU Not Supported

```
Illegal instruction (core dumped)
```

**Check:**
```bash
grep -E "avx2|fma" /proc/cpuinfo
```

CPU ph·∫£i support AVX2 v√† FMA.

### Memory Issues

```
Failed to allocate memory
```

**Solutions:**
- D√πng Small model thay v√¨ Medium
- TƒÉng RAM cho container/VM
- Gi·∫£m batch size

## üìö Additional Resources

- **Whisper C API**: [include/whisper.h](https://github.com/ggerganov/whisper.cpp/blob/master/include/whisper.h)
- **Original Whisper.cpp**: https://github.com/ggerganov/whisper.cpp
- **OpenAI Whisper**: https://github.com/openai/whisper
- **MinIO Documentation**: https://min.io/docs/minio/linux/index.html

## üí¨ Support

N·∫øu g·∫∑p v·∫•n ƒë·ªÅ khi t√≠ch h·ª£p:
1. Ki·ªÉm tra checklist tr√™n
2. Xem logs chi ti·∫øt
3. Verify CPU support v√† dependencies
4. Test v·ªõi sample audio nh·ªè tr∆∞·ªõc

---

**Version**: 1.0  
**Last Updated**: 2025-11-26  
**Maintained by**: Whisper Xeon Builder Team

